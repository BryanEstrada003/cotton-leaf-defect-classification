{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec581d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow<2.11 (from -r ../requirements.txt (line 1))\n",
      "  Downloading tensorflow-2.10.1-cp37-cp37m-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting matplotlib (from -r ../requirements.txt (line 2))\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting pandas (from -r ../requirements.txt (line 3))\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-win_amd64.whl.metadata (12 kB)\n",
      "Collecting scikit-learn (from -r ../requirements.txt (line 4))\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-win_amd64.whl.metadata (10 kB)\n",
      "Collecting numpy (from -r ../requirements.txt (line 5))\n",
      "  Using cached numpy-1.21.6-cp37-cp37m-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin-cidis\\documents\\mai\\cotton-leaf-defect-classification\\.venv\\lib\\site-packages (from tensorflow<2.11->-r ../requirements.txt (line 1)) (24.0)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-win_amd64.whl.metadata (807 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin-cidis\\documents\\mai\\cotton-leaf-defect-classification\\.venv\\lib\\site-packages (from tensorflow<2.11->-r ../requirements.txt (line 1)) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin-cidis\\documents\\mai\\cotton-leaf-defect-classification\\.venv\\lib\\site-packages (from tensorflow<2.11->-r ../requirements.txt (line 1)) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading wrapt-1.16.0-cp37-cp37m-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading grpcio-1.62.3-cp37-cp37m-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.11,>=2.10 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.11,>=2.10.0 (from tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached keras-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r ../requirements.txt (line 2))\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r ../requirements.txt (line 2))\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl.metadata (138 kB)\n",
      "     -------------------------------------- 138.5/138.5 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->-r ../requirements.txt (line 2))\n",
      "  Downloading kiwisolver-1.4.5-cp37-cp37m-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting pillow>=6.2.0 (from matplotlib->-r ../requirements.txt (line 2))\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-win_amd64.whl.metadata (9.7 kB)\n",
      "Collecting pyparsing>=2.2.1 (from matplotlib->-r ../requirements.txt (line 2))\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin-cidis\\documents\\mai\\cotton-leaf-defect-classification\\.venv\\lib\\site-packages (from matplotlib->-r ../requirements.txt (line 2)) (2.9.0.post0)\n",
      "Collecting pytz>=2017.3 (from pandas->-r ../requirements.txt (line 3))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting scipy>=1.1.0 (from scikit-learn->-r ../requirements.txt (line 4))\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting joblib>=0.11 (from scikit-learn->-r ../requirements.txt (line 4))\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->-r ../requirements.txt (line 4))\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached google_auth-2.41.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached importlib_metadata-6.7.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading charset_normalizer-3.4.3-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading MarkupSafe-2.1.5-cp37-cp37m-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached zipp-3.15.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11->-r ../requirements.txt (line 1))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.10.1-cp37-cp37m-win_amd64.whl (455.9 MB)\n",
      "   ---------------------------------------- 455.9/455.9 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.5.3-cp37-cp37m-win_amd64.whl (7.2 MB)\n",
      "   ---------------------------------------- 7.2/7.2 MB 25.6 MB/s eta 0:00:00\n",
      "Downloading pandas-1.3.5-cp37-cp37m-win_amd64.whl (10.0 MB)\n",
      "   ---------------------------------------- 10.0/10.0 MB 26.6 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.0.2-cp37-cp37m-win_amd64.whl (7.1 MB)\n",
      "   ---------------------------------------- 7.1/7.1 MB 25.3 MB/s eta 0:00:00\n",
      "Using cached numpy-1.21.6-cp37-cp37m-win_amd64.whl (14.0 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "   --------------------------------------- 965.4/965.4 kB 20.3 MB/s eta 0:00:00\n",
      "Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.62.3-cp37-cp37m-win_amd64.whl (4.5 MB)\n",
      "   ---------------------------------------- 4.5/4.5 MB 28.6 MB/s eta 0:00:00\n",
      "Downloading h5py-3.8.0-cp37-cp37m-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 2.6/2.6 MB 27.9 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   --------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Downloading kiwisolver-1.4.5-cp37-cp37m-win_amd64.whl (55 kB)\n",
      "   ---------------------------------------- 55.8/55.8 kB 3.0 MB/s eta 0:00:00\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Downloading Pillow-9.5.0-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 2.5/2.5 MB 23.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.19.6-cp37-cp37m-win_amd64.whl (896 kB)\n",
      "   --------------------------------------- 896.6/896.6 kB 18.9 MB/s eta 0:00:00\n",
      "Using cached pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.7.3-cp37-cp37m-win_amd64.whl (34.1 MB)\n",
      "   ---------------------------------------- 34.1/34.1 MB 14.9 MB/s eta 0:00:00\n",
      "Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.8 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Downloading wrapt-1.16.0-cp37-cp37m-win_amd64.whl (37 kB)\n",
      "Using cached google_auth-2.41.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "   ---------------------------------------- 94.2/94.2 kB 5.6 MB/s eta 0:00:00\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "   --------------------------------------- 233.6/233.6 kB 14.9 MB/s eta 0:00:00\n",
      "Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading charset_normalizer-3.4.3-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 53.2/53.2 kB 2.7 MB/s eta 0:00:00\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp37-cp37m-win_amd64.whl (17 kB)\n",
      "Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "   --------------------------------------- 181.3/181.3 kB 10.7 MB/s eta 0:00:00\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 84.9/84.9 kB 4.7 MB/s eta 0:00:00\n",
      "Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, pytz, libclang, keras, flatbuffers, zipp, wrapt, wheel, urllib3, typing-extensions, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyparsing, pyasn1, protobuf, pillow, oauthlib, numpy, MarkupSafe, joblib, idna, grpcio, google-pasta, gast, fonttools, cycler, charset-normalizer, certifi, cachetools, absl-py, werkzeug, scipy, rsa, requests, pyasn1-modules, pandas, opt-einsum, kiwisolver, keras-preprocessing, importlib-metadata, h5py, astunparse, scikit-learn, requests-oauthlib, matplotlib, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.5.2 certifi-2025.8.3 charset-normalizer-3.4.3 cycler-0.11.0 flatbuffers-25.9.23 fonttools-4.38.0 gast-0.4.0 google-auth-2.41.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.62.3 h5py-3.8.0 idna-3.10 importlib-metadata-6.7.0 joblib-1.3.2 keras-2.10.0 keras-preprocessing-1.1.2 kiwisolver-1.4.5 libclang-18.1.1 markdown-3.4.4 matplotlib-3.5.3 numpy-1.21.6 oauthlib-3.2.2 opt-einsum-3.3.0 pandas-1.3.5 pillow-9.5.0 protobuf-3.19.6 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyparsing-3.1.4 pytz-2025.2 requests-2.31.0 requests-oauthlib-2.0.0 rsa-4.9.1 scikit-learn-1.0.2 scipy-1.7.3 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.1 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 threadpoolctl-3.1.0 typing-extensions-4.7.1 urllib3-2.0.7 werkzeug-2.2.3 wheel-0.42.0 wrapt-1.16.0 zipp-3.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a6bc67-251d-4656-9fd2-89041f5e62be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "absl-py                   0.15.0\n",
      "anyio                     4.5.2\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "astunparse                1.6.3\n",
      "async-lru                 2.0.4\n",
      "attrs                     25.3.0\n",
      "babel                     2.17.0\n",
      "backcall                  0.2.0\n",
      "beautifulsoup4            4.13.3\n",
      "bleach                    6.1.0\n",
      "cachetools                5.5.2\n",
      "certifi                   2025.1.31\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.1\n",
      "clang                     5.0\n",
      "comm                      0.2.2\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.13\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "exceptiongroup            1.2.2\n",
      "executing                 2.2.0\n",
      "fastjsonschema            2.21.1\n",
      "flatbuffers               1.12\n",
      "fqdn                      1.5.1\n",
      "gast                      0.4.0\n",
      "google-auth               2.38.0\n",
      "google-auth-oauthlib      1.0.0\n",
      "google-pasta              0.2.0\n",
      "grpcio                    1.70.0\n",
      "h11                       0.14.0\n",
      "h5py                      3.1.0\n",
      "httpcore                  1.0.7\n",
      "httpx                     0.28.1\n",
      "idna                      3.10\n",
      "importlib_metadata        8.5.0\n",
      "importlib_resources       6.4.5\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.12.3\n",
      "ipywidgets                8.1.5\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.6\n",
      "joblib                    1.4.2\n",
      "json5                     0.10.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.2\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.3.6\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.13\n",
      "keras                     2.6.0\n",
      "Keras-Preprocessing       1.1.2\n",
      "kiwisolver                1.4.7\n",
      "llvmlite                  0.39.1\n",
      "Markdown                  3.7\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib                3.3.4\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.1.3\n",
      "mlxtend                   0.23.4\n",
      "nbclient                  0.10.1\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "notebook                  7.3.3\n",
      "notebook_shim             0.2.4\n",
      "numba                     0.56.4\n",
      "numpy                     1.19.5\n",
      "oauthlib                  3.2.2\n",
      "opt-einsum                3.3.0\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandas                    1.2.5\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pickleshare               0.7.5\n",
      "pillow                    10.4.0\n",
      "pip                       25.0.1\n",
      "pkgutil_resolve_name      1.3.10\n",
      "platformdirs              4.3.6\n",
      "prometheus_client         0.21.1\n",
      "prompt_toolkit            3.0.50\n",
      "protobuf                  3.19.6\n",
      "psutil                    7.0.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pyasn1                    0.6.1\n",
      "pyasn1_modules            0.4.1\n",
      "pycparser                 2.22\n",
      "Pygments                  2.19.1\n",
      "pyparsing                 3.1.4\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.3.0\n",
      "pytz                      2025.2\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.3.0\n",
      "referencing               0.35.1\n",
      "requests                  2.32.3\n",
      "requests-oauthlib         2.0.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.20.1\n",
      "rsa                       4.9\n",
      "scikit-learn              1.3.2\n",
      "scipy                     1.10.1\n",
      "seaborn                   0.11.2\n",
      "Send2Trash                1.8.3\n",
      "setuptools                56.0.0\n",
      "six                       1.15.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "stack-data                0.6.3\n",
      "tensorboard               2.14.0\n",
      "tensorboard-data-server   0.7.2\n",
      "tensorflow                2.6.0\n",
      "tensorflow-estimator      2.15.0\n",
      "termcolor                 1.1.0\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.5.0\n",
      "tinycss2                  1.2.1\n",
      "tomli                     2.2.1\n",
      "tornado                   6.4.2\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20241206\n",
      "typing_extensions         4.13.0\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.3\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.8.0\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "Werkzeug                  3.0.6\n",
      "wheel                     0.45.1\n",
      "widgetsnbextension        4.0.13\n",
      "wrapt                     1.12.1\n",
      "zipp                      3.20.2\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e77e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.6.0\n",
      "Keras Version: 2.6.0\n",
      "\n",
      "Python 3.8.12 (default, Mar 27 2025, 14:46:10) \n",
      "[GCC 7.5.0]\n",
      "Pandas 1.2.5\n",
      "Scikit-Learn 1.3.2\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 08:42:42.874206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:b3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-09-30 08:42:42.933499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:b3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-09-30 08:42:42.934541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:b3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import keras\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices(\"GPU\")) > 0\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b16d9-1a64-4123-84dc-84f7fae21aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMILLA\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade2a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "forma_entrada = (224, 224, 3)\n",
    "tamaño_lote = 64\n",
    "épocas = 60\n",
    "initial_learning_rate = 1e-3\n",
    "\n",
    "preprocess_fn = tf.keras.applications.vgg16.preprocess_input\n",
    "\n",
    "\n",
    "# Rutas\n",
    "ruta_entrenamiento = \"./datasets/2D/data_augmentation/color_2/train\"\n",
    "ruta_validacion = \"./datasets/2D/data_augmentation/color_2/validation\"\n",
    "ruta_test = \"./datasets/2D/data_augmentation/color_2/test\"\n",
    "\n",
    "# ruta_entrenamiento = './datasets/order_data/color/train'\n",
    "# ruta_validacion = './datasets/order_data/color/validation'\n",
    "# ruta_test = './datasets/order_data/color/test'\n",
    "\n",
    "# Generadores\n",
    "datagen = ImageDataGenerator(\n",
    "    # preprocessing_function=preprocess_fn,\n",
    "    rescale=1.0\n",
    "    / 255\n",
    ")\n",
    "datagen_valid = ImageDataGenerator(\n",
    "    # preprocessing_function=preprocess_fn,\n",
    "    rescale=1.0\n",
    "    / 255\n",
    ")  # ANTES APLIQUE RESCALE\n",
    "datagen_test = ImageDataGenerator(\n",
    "    # preprocessing_function=preprocess_fn,\n",
    "    rescale=1.0\n",
    "    / 255\n",
    ")\n",
    "\n",
    "generador_entrenamiento = datagen.flow_from_directory(\n",
    "    ruta_entrenamiento,\n",
    "    target_size=forma_entrada[:2],\n",
    "    batch_size=tamaño_lote,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "generador_validacion = datagen_valid.flow_from_directory(\n",
    "    ruta_validacion,\n",
    "    target_size=forma_entrada[:2],\n",
    "    batch_size=tamaño_lote,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "generador_test = datagen_test.flow_from_directory(\n",
    "    ruta_test,\n",
    "    target_size=forma_entrada[:2],\n",
    "    batch_size=tamaño_lote,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d22fe-4c03-4573-92a2-7646a77a3d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = next(generador_entrenamiento)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)  # Nota: i+1 para que los índices empiecen en 1\n",
    "    plt.imshow(X[i])  # Añade cmap='gray' para imágenes en escala de grises\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Ejemplos con Cutout aplicado aleatoriamente\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ef635-2d41-4cff-847b-d678eb5e14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(generador_entrenamiento.filepaths) // generador_entrenamiento.batch_size\n",
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba65bd-9b0a-4133-b8fd-76aa70e2a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### AÑADIR LR DECAY FACTOR #####\n",
    "# learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=initial_learning_rate,\n",
    "#     decay_steps=30*steps,  # Decaimiento más frecuente\n",
    "#     decay_rate=0.9,  # Decaimiento más fuerte\n",
    "#     staircase=True)\n",
    "learning_rate = initial_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55500240-0ab2-43e2-93e0-3d767a26d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74e475-8444-4f68-88d3-a205f71660a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest = \"models/VGG16/siamese_model_vgg16_apple_data_REF.h5\"\n",
    "\n",
    "# base_model = tf.keras.models.load_model(latest, compile=False)\n",
    "\n",
    "# base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e554240-a000-4fa3-a3b4-be5e2fedc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extraer solo la parte VGG16 del modelo cargado\n",
    "# vgg_layer = base_model.get_layer('vgg16')  # Asumiendo que se llama 'vgg16' en tu modelo cargado\n",
    "# vgg_layer.trainable = False  # Congelar pesos\n",
    "\n",
    "# entrada = layers.Input(shape=forma_entrada)\n",
    "# x = vgg_layer(entrada)  # Usar la VGG16 cargada\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "# x = layers.Dropout(0.75)(x)\n",
    "# salida = layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "# # 3. Compilar\n",
    "# model = Model(inputs=entrada, outputs=salida)\n",
    "# optimizador = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# model.compile(\n",
    "#     loss='categorical_crossentropy',\n",
    "#     optimizer=optimizador,\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "# #\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66932579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construccion del modelo\n",
    "# --------------------------------------------------------------------------\n",
    "def model_init(learning_rate, dropout_rate, l2_reg):\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=forma_entrada)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    entrada = layers.Input(shape=forma_entrada)\n",
    "    x = base_model(entrada)\n",
    "    x = layers.Flatten()(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = layers.Dense(\n",
    "        256, activation=\"relu\", kernel_regularizer=regularizers.l2(l2_reg)\n",
    "    )(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    salida = layers.Dense(4, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=entrada, outputs=salida)\n",
    "    optimizador = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=optimizador, metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521c5c8-9dbc-4219-892b-a0c94ca938f2",
   "metadata": {},
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 1. Definir función objetivo a optimizar\n",
    "\n",
    "def optimize_model(lr, dropout_rate, l2_reg, batch_size): # Convertir parámetros\n",
    "lr = 10**lr # Usar escala logarítmica\n",
    "l2_reg = 10**l2_reg\n",
    "batch_size = int(batch_size)\n",
    "\n",
    "    # Construir modelo con hiperparámetros actuales\n",
    "    modelo = model_init(\n",
    "        learning_rate=lr,\n",
    "        dropout_rate=dropout_rate,\n",
    "        l2_reg=l2_reg\n",
    "    )\n",
    "\n",
    "    # Entrenamiento reducido para evaluación rápida\n",
    "    history = modelo.fit(\n",
    "        generador_entrenamiento_combinado,\n",
    "        #steps_per_epoch=train_samples // tamaño_lote,  # Subconjunto para optimización\n",
    "        validation_data=generador_validacion_combinado,\n",
    "        #validation_steps=val_samples // tamaño_lote,\n",
    "        epochs=3,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Retornar el mejor valor de val_accuracy\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# 2. Definir espacio de búsqueda\n",
    "\n",
    "pbounds = {\n",
    "'lr': (-5, -3), # 10^-5 a 10^-3\n",
    "'dropout_rate': (0.3, 0.7),\n",
    "'l2_reg': (-5, -2), # 10^-5 a 10^-2\n",
    "'batch_size': (16, 64) # Discretizar después\n",
    "}\n",
    "\n",
    "# 3. Crear optimizador bayesiano\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "f=optimize_model,\n",
    "pbounds=pbounds,\n",
    "random_state=42,\n",
    "verbose=1\n",
    ")\n",
    "\n",
    "# 4. Ejecutar optimización\n",
    "\n",
    "optimizer.maximize(\n",
    "init_points=10, # Exploración inicial aleatoria\n",
    "n_iter=2, # Iteraciones bayesianas\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392a7f6-232e-47da-9480-5d5c98b6bc61",
   "metadata": {},
   "source": [
    "# Obtener mejores parámetros\n",
    "\n",
    "best_params = optimizer.max['params']\n",
    "best_params['batch_size'] = int(best_params['batch_size']) # Discretizar\n",
    "\n",
    "# Construir modelo final con mejores parámetros\n",
    "\n",
    "model = model_init(\n",
    "learning_rate=10**best_params['lr'],\n",
    "dropout_rate=best_params['dropout_rate'],\n",
    "l2_reg=10**best_params['l2_reg']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52f6c1-daa1-41b7-af47-26d5648e2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Definir función objetivo a optimizar\n",
    "def optimize_model(lr, dropout_rate, l2_reg):\n",
    "    # Convertir parámetros\n",
    "    lr = 10**lr  # Usar escala logarítmica\n",
    "    l2_reg = 10**l2_reg\n",
    "\n",
    "    # Construir modelo con hiperparámetros actuales\n",
    "    modelo = model_init(learning_rate=lr, dropout_rate=dropout_rate, l2_reg=l2_reg)\n",
    "\n",
    "    # Entrenamiento reducido para evaluación rápida\n",
    "    history = modelo.fit(\n",
    "        generador_entrenamiento,\n",
    "        # steps_per_epoch=train_samples // tamaño_lote,  # Subconjunto para optimización\n",
    "        validation_data=generador_validacion,\n",
    "        # validation_steps=val_samples // tamaño_lote,\n",
    "        epochs=3,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Retornar el mejor valor de val_accuracy\n",
    "    return np.max(history.history[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "# 2. Definir espacio de búsqueda\n",
    "pbounds = {\n",
    "    \"lr\": (-5, -2),  # 10^-5 a 10^-3\n",
    "    \"dropout_rate\": (0.7, 0.9),\n",
    "    \"l2_reg\": (-5, -2),  # 10^-5 a 10^-2\n",
    "}\n",
    "\n",
    "# 3. Crear optimizador bayesiano\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_model, pbounds=pbounds, random_state=42, verbose=1\n",
    ")\n",
    "\n",
    "# 4. Ejecutar optimización\n",
    "optimizer.maximize(\n",
    "    init_points=2,  # Exploración inicial aleatoria\n",
    "    n_iter=10,  # Iteraciones bayesianas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481bfd4-832d-4ee1-98fe-a7c2a279e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a1a22-41ef-455f-b778-98463e55f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener mejores parámetros\n",
    "best_params = optimizer.max[\"params\"]\n",
    "\n",
    "# Construir modelo final con mejores parámetros\n",
    "model = model_init(\n",
    "    learning_rate=10 ** best_params[\"lr\"],\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    "    l2_reg=10 ** best_params[\"l2_reg\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4dac26-153d-495c-9b3d-31460d326b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_init(learning_rate, 0.7, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02534b1f-8481-4cbc-957f-0fcc145ec06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de callbacks\n",
    "import os\n",
    "\n",
    "path_models = \"./models/pruebas/VGG16_RGB_bayesian_batchdef/\"\n",
    "os.makedirs(path_models, exist_ok=True)\n",
    "arch = \"VGG16_SINGLE_INPUT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5f3bc-9f78-4f97-8888-560a441e0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta donde se guardará el archivo (ej: 'C:/Users/tu_usuario/proyecto/resultados/hiperparametros.txt')\n",
    "ruta_archivo = path_models + \"hiperparametros.txt\"  # ¡Cambia esto!\n",
    "\n",
    "# Asegurarse de que la carpeta exista (si no, la crea)\n",
    "os.makedirs(os.path.dirname(ruta_archivo), exist_ok=True)\n",
    "\n",
    "# Guardar en formato legible (clave = valor)\n",
    "with open(ruta_archivo, \"w\") as f:\n",
    "    for key, value in best_params.items():\n",
    "        f.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "print(f\"¡Hiperparámetros guardados en: {ruta_archivo}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a42996-7df4-4067-8b04-3cc2cab88702",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    path_models + \"model_single_input_silhouette_0_0.h5\",\n",
    "    monitor=\"val_loss\",  # <-- Guarda el mejor modelo basado en val_loss (más estable que val_accuracy)\n",
    "    verbose=1,\n",
    "    save_best_only=True,  # guarda cuando haya mejoras\n",
    "    save_weights_only=True,\n",
    "    mode=\"min\",  # estaba en auto\n",
    "    period=10,  # Guarda pesos cada 10 épocas\n",
    ")\n",
    "\n",
    "early = EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    min_delta=0.001,\n",
    "    patience=10,  # <-- Detén el entrenamiento cuando val_accuracy no mejore en 10 épocas\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    "    mode=\"max\",\n",
    ")\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",  # Monitorear pérdida de validación\n",
    "    mode=\"min\",  # Reducir LR cuando val_loss deje de disminuir\n",
    "    factor=0.5,  # Reducción moderada del LR\n",
    "    patience=5,  # Esperar 5 épocas sin mejora\n",
    "    verbose=1,\n",
    "    min_lr=1e-6,  # LR mínimo permitido+\n",
    ")\n",
    "\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# lr_scheduler = ReduceLROnPlateau(\n",
    "#     monitor='val_accuracy',\n",
    "#     factor=0.5,  # Reducir LR a la mitad\n",
    "#     patience=10,  # Esperar 10 épocas sin mejora\n",
    "#     min_lr=1e-6,  # LR mínimo\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# Añadir a los callbacks\n",
    "callbacks = [checkpoint, reduce_lr, early]  # <-- Añadido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531117e4-8e45-49fd-aa90-302c9a1237b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para TensorFlow 2.6 (compatible con CUDA 11.2 y cuDNN 8.1)\n",
    "# %pip uninstall tensorflow -y\n",
    "# %pip install tensorflow-gpu==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad9843-5048-416a-956f-9202172a316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "historia = model.fit(\n",
    "    generador_entrenamiento,\n",
    "    # steps_per_epoch=generador_entrenamiento.samples // tamaño_lote,\n",
    "    epochs=épocas,\n",
    "    validation_data=generador_validacion,\n",
    "    # validation_steps=generador_validacion.samples // tamaño_lote,\n",
    "    callbacks=callbacks,\n",
    "    # class_weight=class_weights  # <-- Añadir esto\n",
    ")\n",
    "\n",
    "model.save(path_models + arch + \"/model_single_input_silhouette_0_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb965b3e-317f-41dd-83d4-3a88d4b2add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_lr = model.optimizer.lr.numpy()\n",
    "print(f\"Learning rate actual: {current_lr}\")\n",
    "\n",
    "# Graficar el learning rate a lo largo de las épocas\n",
    "plt.plot(historia.history[\"lr\"])\n",
    "plt.title(\"Learning Rate durante el entrenamiento\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"LR\")\n",
    "plt.savefig(path_models + \"VGG16_silhouette_0_0_lr_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff29f4-5824-42a8-80d2-e19d76b89623",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_models + arch + \"/model_single_input_silhouette_0_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de Pérdida\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(historia.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(historia.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Plot\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(\n",
    "    path_models + \"VGG16_silhouette_0_0_loss_plot.png\"\n",
    ")  # Guarda el gráfico en loss_plot.png\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de Precisión\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(historia.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(historia.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Plot\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(\n",
    "    path_models + \"VGG16_silhouette_0_0_accuracy_plot.png\"\n",
    ")  # Guarda el gráfico en accuracy_plot.png\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a21389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # #### USAR PESOS YA GUARDADOS\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# latest = path_models + arch + '/model_single_input_silhouette_0_0.h5'\n",
    "# modelo = tf.keras.models.load_model(latest)\n",
    "# modelo.summary()\n",
    "\n",
    "# # Evaluación sobre el set de prueba\n",
    "# y_pred = modelo.predict(generador_test, verbose=1)\n",
    "# predicted_labels = np.argmax(y_pred, axis=1)\n",
    "# true_labels = generador_test.classes\n",
    "\n",
    "\n",
    "# categorias = ['Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "\n",
    "# #class_names = [\"clase_1\", \"clase_2\", \"clase_3\", \"clase_4\"]\n",
    "# # class_names = [\"clase_1\", \"clase_2\"]\n",
    "# # Calcular métricas\n",
    "# print('Accuracy:', accuracy_score(true_labels, y_pred))\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Suponiendo que tienes:\n",
    "# # verdaderas: etiquetas verdaderas\n",
    "# # predichas: etiquetas predichas\n",
    "# # categorias: nombres de las clases\n",
    "\n",
    "# # Generar el reporte\n",
    "# reporte = classification_report(true_labels, y_pred, target_names=categorias)\n",
    "# print(reporte)\n",
    "# # Guardar en un archivo .txt\n",
    "# with open(path_models+'reporte_clasificacion.txt', 'w') as archivo:\n",
    "#     archivo.write(reporte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d7755-86a9-4c62-b41c-0dedf3d716ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Check Current Learning Rate######\n",
    "\n",
    "# current_lr = learning_rate(model.optimizer.iterations)\n",
    "# print(f\"Current learning rate: {current_lr.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluación sobre el set de prueba\n",
    "y_pred = model.predict(generador_test, verbose=1)\n",
    "predicted_labels = np.argmax(y_pred, axis=1)\n",
    "true_labels = generador_test.classes\n",
    "\n",
    "# Calcular métricas\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average=None)\n",
    "recall = recall_score(true_labels, predicted_labels, average=None)\n",
    "f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "\n",
    "categories = [\"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\"]\n",
    "\n",
    "# Crear la gráfica de la matriz de confusión\n",
    "sns.set(font_scale=1.4)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"g\",\n",
    "    cmap=\"Pastel1_r\",\n",
    "    xticklabels=categories,\n",
    "    yticklabels=categories,\n",
    ")\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\n",
    "    path_models + \"VGG16_silhouette_0_0_confusion_matrix.png\"\n",
    ")  # Guarda la imagen en un archivo PNG\n",
    "plt.close()  # Cierra la figura\n",
    "\n",
    "class_counts = cm.sum(axis=1)  # número total de instancias reales por clase\n",
    "diag = np.diag(cm)  # verdaderos positivos por clase\n",
    "class_accuracy = diag / class_counts\n",
    "\n",
    "# Tabla en pandas\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Clase\": categories,\n",
    "        \"Accuracy\": class_accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Imprimir en pantalla (opcional)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(df)\n",
    "print(\"Average Precision:\", df[\"Precision\"].mean())\n",
    "print(\"Average Recall:\", df[\"Recall\"].mean())\n",
    "print(\"Average F1-Score:\", df[\"F1-Score\"].mean())\n",
    "\n",
    "# Guardar las métricas en un archivo de texto\n",
    "with open(path_models + \"VGG16_silhouette_0_0_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Accuracy: {accuracy}\\n\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(np.array2string(cm))\n",
    "    f.write(\"\\n\\nMetrics Table:\\n\")\n",
    "    f.write(df.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(f\"Average Precision: {df['Precision'].mean()}\\n\")\n",
    "    f.write(f\"Average Recall: {df['Recall'].mean()}\\n\")\n",
    "    f.write(f\"Average F1-Score: {df['F1-Score'].mean()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
