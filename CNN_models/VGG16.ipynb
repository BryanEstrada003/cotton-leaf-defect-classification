{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import keras\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     # Asignar 8GB a cada modelo (24GB total / 3 modelos)\n",
    "#     tf.config.set_logical_device_configuration(\n",
    "#         gpus[0],\n",
    "#         [tf.config.LogicalDeviceConfiguration(memory_limit=10000)]  # 8GB en MB\n",
    "#     )\n",
    "#     print(\"GPU limitada a 10GB para este notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b16d9-1a64-4123-84dc-84f7fae21aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMILLA\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade2a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "forma_entrada = (224, 224, 3)\n",
    "tamaño_lote = 64\n",
    "épocas = 60\n",
    "initial_learning_rate = 1e-3\n",
    "\n",
    "preprocess_fn = tf.keras.applications.vgg16.preprocess_input\n",
    "\n",
    "\n",
    "# Rutas\n",
    "ruta_entrenamiento = './datasets/2D/data_augmentation/color_2/train'\n",
    "ruta_validacion = './datasets/2D/data_augmentation/color_2/validation'\n",
    "ruta_test = './datasets/2D/data_augmentation/color_2/test'\n",
    "\n",
    "# ruta_entrenamiento = './datasets/order_data/color/train'\n",
    "# ruta_validacion = './datasets/order_data/color/validation'\n",
    "# ruta_test = './datasets/order_data/color/test'\n",
    "\n",
    "# Generadores\n",
    "datagen = ImageDataGenerator(\n",
    "    # preprocessing_function=preprocess_fn,\n",
    "    rescale=1./255\n",
    ")\n",
    "datagen_valid = ImageDataGenerator(\n",
    "    # preprocessing_function=preprocess_fn,\n",
    "    rescale=1./255\n",
    ") # ANTES APLIQUE RESCALE\n",
    "datagen_test = ImageDataGenerator(\n",
    "    # preprocessing_function=preprocess_fn,\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "generador_entrenamiento = datagen.flow_from_directory(\n",
    "    ruta_entrenamiento,\n",
    "    target_size=forma_entrada[:2],\n",
    "    batch_size=tamaño_lote,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "generador_validacion = datagen_valid.flow_from_directory(\n",
    "    ruta_validacion,\n",
    "    target_size=forma_entrada[:2],\n",
    "    batch_size=tamaño_lote,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "generador_test = datagen_test.flow_from_directory(\n",
    "    ruta_test,\n",
    "    target_size=forma_entrada[:2],\n",
    "    batch_size=tamaño_lote,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d22fe-4c03-4573-92a2-7646a77a3d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = next(generador_entrenamiento)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)  # Nota: i+1 para que los índices empiecen en 1\n",
    "    plt.imshow(X[i])  # Añade cmap='gray' para imágenes en escala de grises\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Ejemplos con Cutout aplicado aleatoriamente')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ef635-2d41-4cff-847b-d678eb5e14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = len(generador_entrenamiento.filepaths) // generador_entrenamiento.batch_size\n",
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba65bd-9b0a-4133-b8fd-76aa70e2a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### AÑADIR LR DECAY FACTOR #####\n",
    "# learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=initial_learning_rate,\n",
    "#     decay_steps=30*steps,  # Decaimiento más frecuente\n",
    "#     decay_rate=0.9,  # Decaimiento más fuerte\n",
    "#     staircase=True)\n",
    "learning_rate = initial_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55500240-0ab2-43e2-93e0-3d767a26d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74e475-8444-4f68-88d3-a205f71660a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest = \"models/VGG16/siamese_model_vgg16_apple_data_REF.h5\"\n",
    "\n",
    "# base_model = tf.keras.models.load_model(latest, compile=False)\n",
    "\n",
    "# base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e554240-a000-4fa3-a3b4-be5e2fedc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extraer solo la parte VGG16 del modelo cargado\n",
    "# vgg_layer = base_model.get_layer('vgg16')  # Asumiendo que se llama 'vgg16' en tu modelo cargado\n",
    "# vgg_layer.trainable = False  # Congelar pesos\n",
    "\n",
    "# entrada = layers.Input(shape=forma_entrada)\n",
    "# x = vgg_layer(entrada)  # Usar la VGG16 cargada\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "# x = layers.Dropout(0.75)(x)\n",
    "# salida = layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "# # 3. Compilar\n",
    "# model = Model(inputs=entrada, outputs=salida)\n",
    "# optimizador = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# model.compile(\n",
    "#     loss='categorical_crossentropy', \n",
    "#     optimizer=optimizador, \n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "# #\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66932579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construccion del modelo\n",
    "# --------------------------------------------------------------------------\n",
    "def model_init(learning_rate, dropout_rate, l2_reg):\n",
    "    base_model = VGG16(\n",
    "        weights='imagenet', \n",
    "        include_top=False, \n",
    "        input_shape=forma_entrada\n",
    "    )\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    entrada = layers.Input(shape=forma_entrada)\n",
    "    x = base_model(entrada)\n",
    "    x = layers.Flatten()(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = layers.Dense(\n",
    "        256,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(l2_reg)\n",
    "    )(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    salida = layers.Dense(4, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=entrada, outputs=salida)\n",
    "    optimizador = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=optimizador, \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521c5c8-9dbc-4219-892b-a0c94ca938f2",
   "metadata": {},
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 1. Definir función objetivo a optimizar\n",
    "def optimize_model(lr, dropout_rate, l2_reg, batch_size):\n",
    "    # Convertir parámetros\n",
    "    lr = 10**lr  # Usar escala logarítmica\n",
    "    l2_reg = 10**l2_reg\n",
    "    batch_size = int(batch_size)\n",
    "    \n",
    "    # Construir modelo con hiperparámetros actuales\n",
    "    modelo = model_init(\n",
    "        learning_rate=lr,\n",
    "        dropout_rate=dropout_rate,\n",
    "        l2_reg=l2_reg\n",
    "    )\n",
    "    \n",
    "    # Entrenamiento reducido para evaluación rápida\n",
    "    history = modelo.fit(\n",
    "        generador_entrenamiento_combinado,\n",
    "        #steps_per_epoch=train_samples // tamaño_lote,  # Subconjunto para optimización\n",
    "        validation_data=generador_validacion_combinado,\n",
    "        #validation_steps=val_samples // tamaño_lote,\n",
    "        epochs=3,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Retornar el mejor valor de val_accuracy\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# 2. Definir espacio de búsqueda\n",
    "pbounds = {\n",
    "    'lr': (-5, -3),       # 10^-5 a 10^-3\n",
    "    'dropout_rate': (0.3, 0.7),\n",
    "    'l2_reg': (-5, -2),   # 10^-5 a 10^-2\n",
    "    'batch_size': (16, 64) # Discretizar después\n",
    "}\n",
    "\n",
    "# 3. Crear optimizador bayesiano\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_model,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. Ejecutar optimización\n",
    "optimizer.maximize(\n",
    "    init_points=10,  # Exploración inicial aleatoria\n",
    "    n_iter=2,      # Iteraciones bayesianas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392a7f6-232e-47da-9480-5d5c98b6bc61",
   "metadata": {},
   "source": [
    "# Obtener mejores parámetros\n",
    "best_params = optimizer.max['params']\n",
    "best_params['batch_size'] = int(best_params['batch_size'])  # Discretizar\n",
    "\n",
    "# Construir modelo final con mejores parámetros\n",
    "model = model_init(\n",
    "    learning_rate=10**best_params['lr'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    l2_reg=10**best_params['l2_reg']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52f6c1-daa1-41b7-af47-26d5648e2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 1. Definir función objetivo a optimizar\n",
    "def optimize_model(lr, dropout_rate, l2_reg):\n",
    "    # Convertir parámetros\n",
    "    lr = 10**lr  # Usar escala logarítmica\n",
    "    l2_reg = 10**l2_reg\n",
    "\n",
    "    # Construir modelo con hiperparámetros actuales\n",
    "    modelo = model_init(\n",
    "        learning_rate=lr,\n",
    "        dropout_rate=dropout_rate,\n",
    "        l2_reg=l2_reg\n",
    "    )\n",
    "    \n",
    "    # Entrenamiento reducido para evaluación rápida\n",
    "    history = modelo.fit(\n",
    "        generador_entrenamiento,\n",
    "        # steps_per_epoch=train_samples // tamaño_lote,  # Subconjunto para optimización\n",
    "        validation_data=generador_validacion,\n",
    "        # validation_steps=val_samples // tamaño_lote,\n",
    "        epochs=3,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Retornar el mejor valor de val_accuracy\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# 2. Definir espacio de búsqueda\n",
    "pbounds = {\n",
    "    'lr': (-5, -2),       # 10^-5 a 10^-3\n",
    "    'dropout_rate': (0.7, 0.9),\n",
    "    'l2_reg': (-5, -2),   # 10^-5 a 10^-2\n",
    "}\n",
    "\n",
    "# 3. Crear optimizador bayesiano\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_model,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. Ejecutar optimización\n",
    "optimizer.maximize(\n",
    "    init_points=2,  # Exploración inicial aleatoria\n",
    "    n_iter=10,      # Iteraciones bayesianas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481bfd4-832d-4ee1-98fe-a7c2a279e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a1a22-41ef-455f-b778-98463e55f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener mejores parámetros\n",
    "best_params = optimizer.max['params']\n",
    "\n",
    "# Construir modelo final con mejores parámetros\n",
    "model = model_init(\n",
    "    learning_rate=10**best_params['lr'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    l2_reg=10**best_params['l2_reg']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4dac26-153d-495c-9b3d-31460d326b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model_init(learning_rate, 0.7, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02534b1f-8481-4cbc-957f-0fcc145ec06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de callbacks\n",
    "import os\n",
    "path_models = './models/pruebas/VGG16_RGB_bayesian_batchdef/'\n",
    "os.makedirs(path_models, exist_ok=True)\n",
    "arch = 'VGG16_SINGLE_INPUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5f3bc-9f78-4f97-8888-560a441e0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta donde se guardará el archivo (ej: 'C:/Users/tu_usuario/proyecto/resultados/hiperparametros.txt')\n",
    "ruta_archivo = path_models + \"hiperparametros.txt\"  # ¡Cambia esto!\n",
    "\n",
    "# Asegurarse de que la carpeta exista (si no, la crea)\n",
    "os.makedirs(os.path.dirname(ruta_archivo), exist_ok=True)\n",
    "\n",
    "# Guardar en formato legible (clave = valor)\n",
    "with open(ruta_archivo, \"w\") as f:\n",
    "    for key, value in best_params.items():\n",
    "        f.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "print(f\"¡Hiperparámetros guardados en: {ruta_archivo}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a42996-7df4-4067-8b04-3cc2cab88702",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    path_models + \"model_single_input_silhouette_0_0.h5\",\n",
    "    monitor='val_loss', # <-- Guarda el mejor modelo basado en val_loss (más estable que val_accuracy)\n",
    "    verbose=1,\n",
    "    save_best_only=True, # guarda cuando haya mejoras\n",
    "    save_weights_only=True,\n",
    "    mode='min', #estaba en auto\n",
    "    period=10  # Guarda pesos cada 10 épocas\n",
    ")\n",
    "\n",
    "early = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0.001, \n",
    "    patience=10,  # <-- Detén el entrenamiento cuando val_accuracy no mejore en 10 épocas\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Monitorear pérdida de validación\n",
    "    mode='min',         # Reducir LR cuando val_loss deje de disminuir\n",
    "    factor=0.5,         # Reducción moderada del LR\n",
    "    patience=5,        # Esperar 5 épocas sin mejora\n",
    "    verbose=1,\n",
    "    min_lr=1e-6         # LR mínimo permitido+\n",
    ")\n",
    "\n",
    "# from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# lr_scheduler = ReduceLROnPlateau(\n",
    "#     monitor='val_accuracy',\n",
    "#     factor=0.5,  # Reducir LR a la mitad\n",
    "#     patience=10,  # Esperar 10 épocas sin mejora\n",
    "#     min_lr=1e-6,  # LR mínimo\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# Añadir a los callbacks\n",
    "callbacks = [checkpoint, reduce_lr, early]  # <-- Añadido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531117e4-8e45-49fd-aa90-302c9a1237b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para TensorFlow 2.6 (compatible con CUDA 11.2 y cuDNN 8.1)\n",
    "# %pip uninstall tensorflow -y\n",
    "# %pip install tensorflow-gpu==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad9843-5048-416a-956f-9202172a316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "historia = model.fit(\n",
    "    generador_entrenamiento,\n",
    "    #steps_per_epoch=generador_entrenamiento.samples // tamaño_lote,\n",
    "    epochs=épocas,\n",
    "    validation_data=generador_validacion,\n",
    "    #validation_steps=generador_validacion.samples // tamaño_lote,\n",
    "    callbacks=callbacks,\n",
    "    # class_weight=class_weights  # <-- Añadir esto\n",
    ")\n",
    "\n",
    "model.save(path_models + arch + '/model_single_input_silhouette_0_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb965b3e-317f-41dd-83d4-3a88d4b2add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_lr = model.optimizer.lr.numpy()\n",
    "print(f\"Learning rate actual: {current_lr}\")\n",
    "\n",
    "# Graficar el learning rate a lo largo de las épocas\n",
    "plt.plot(historia.history['lr'])\n",
    "plt.title('Learning Rate durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('LR')\n",
    "plt.savefig(path_models +\"VGG16_silhouette_0_0_lr_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff29f4-5824-42a8-80d2-e19d76b89623",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_models + arch + '/model_single_input_silhouette_0_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de Pérdida\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(historia.history['loss'], label='Training Loss')\n",
    "plt.plot(historia.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss Plot\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(path_models +\"VGG16_silhouette_0_0_loss_plot.png\")  # Guarda el gráfico en loss_plot.png\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de Precisión\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(historia.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(historia.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Plot\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(path_models +\"VGG16_silhouette_0_0_accuracy_plot.png\")  # Guarda el gráfico en accuracy_plot.png\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a21389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # #### USAR PESOS YA GUARDADOS\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# latest = path_models + arch + '/model_single_input_silhouette_0_0.h5'\n",
    "# modelo = tf.keras.models.load_model(latest)\n",
    "# modelo.summary()\n",
    "\n",
    "# # Evaluación sobre el set de prueba\n",
    "# y_pred = modelo.predict(generador_test, verbose=1)\n",
    "# predicted_labels = np.argmax(y_pred, axis=1)\n",
    "# true_labels = generador_test.classes\n",
    "\n",
    "\n",
    "# categorias = ['Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "\n",
    "# #class_names = [\"clase_1\", \"clase_2\", \"clase_3\", \"clase_4\"]\n",
    "# # class_names = [\"clase_1\", \"clase_2\"]\n",
    "# # Calcular métricas\n",
    "# print('Accuracy:', accuracy_score(true_labels, y_pred))\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Suponiendo que tienes:\n",
    "# # verdaderas: etiquetas verdaderas\n",
    "# # predichas: etiquetas predichas\n",
    "# # categorias: nombres de las clases\n",
    "\n",
    "# # Generar el reporte\n",
    "# reporte = classification_report(true_labels, y_pred, target_names=categorias)\n",
    "# print(reporte)\n",
    "# # Guardar en un archivo .txt\n",
    "# with open(path_models+'reporte_clasificacion.txt', 'w') as archivo:\n",
    "#     archivo.write(reporte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d7755-86a9-4c62-b41c-0dedf3d716ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Check Current Learning Rate######\n",
    "\n",
    "#current_lr = learning_rate(model.optimizer.iterations)\n",
    "#print(f\"Current learning rate: {current_lr.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluación sobre el set de prueba\n",
    "y_pred = model.predict(generador_test, verbose=1)\n",
    "predicted_labels = np.argmax(y_pred, axis=1)\n",
    "true_labels = generador_test.classes\n",
    "\n",
    "# Calcular métricas\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average=None)\n",
    "recall = recall_score(true_labels, predicted_labels, average=None)\n",
    "f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "\n",
    "categories = ['Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "\n",
    "# Crear la gráfica de la matriz de confusión\n",
    "sns.set(font_scale=1.4)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Pastel1_r', xticklabels=categories, yticklabels=categories)\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig(path_models + \"VGG16_silhouette_0_0_confusion_matrix.png\")  # Guarda la imagen en un archivo PNG\n",
    "plt.close()  # Cierra la figura\n",
    "\n",
    "class_counts = cm.sum(axis=1)  # número total de instancias reales por clase\n",
    "diag = np.diag(cm)             # verdaderos positivos por clase\n",
    "class_accuracy = diag / class_counts\n",
    "\n",
    "# Tabla en pandas\n",
    "df = pd.DataFrame({\n",
    "    'Clase': categories,\n",
    "    'Accuracy': class_accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1\n",
    "})\n",
    "\n",
    "# Imprimir en pantalla (opcional)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(df)\n",
    "print(\"Average Precision:\", df[\"Precision\"].mean())\n",
    "print(\"Average Recall:\", df[\"Recall\"].mean())\n",
    "print(\"Average F1-Score:\", df[\"F1-Score\"].mean())\n",
    "\n",
    "# Guardar las métricas en un archivo de texto\n",
    "with open(path_models + \"VGG16_silhouette_0_0_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Accuracy: {accuracy}\\n\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(np.array2string(cm))\n",
    "    f.write(\"\\n\\nMetrics Table:\\n\")\n",
    "    f.write(df.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(f\"Average Precision: {df['Precision'].mean()}\\n\")\n",
    "    f.write(f\"Average Recall: {df['Recall'].mean()}\\n\")\n",
    "    f.write(f\"Average F1-Score: {df['F1-Score'].mean()}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
